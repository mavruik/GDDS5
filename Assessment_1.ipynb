{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5201: Assessment 1\n",
    "## The Elements of Machine Learning\n",
    "\n",
    "### Objectives\n",
    "This assignment consists of three parts (A,B,C) that assess your understanding of model complexity, model selection, uncertainty in prediction with bootstrapping, and probabilistic machine learning. The total marks of this assessment is 100, and will contribute to the 20% of your final score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Note\n",
    "* You can complete your assignment using the codes shared in the unit as a base. However, <font color='red'>**you should make sure the codes you are borrowing are correct and relevant to the question**</font>.\n",
    "\n",
    "* Please follow the structure of this template as much as you can.\n",
    "\n",
    "* You can use the prepopulated codes cells or change them if you prefere. However, please do not change the name of the key variables, functions, and parameters eg `knn`, `num.fold`, `train.data`. It helps us to read and understand your submissiont more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A.  Model Complexity and Model Selection\n",
    "In this part, you study the effect of model complexity on the training and testing errors.  You also demonstrate your programming skills by developing a regression algorithm and a cross-validation technique that will be used to select the models with the most effective complexity.\n",
    "\n",
    "__Background__. A KNN regressor is similar to a KNN classifier (covered in Activity 1.1) in the sence that it finds the K nearest neighbors and estimates the label of the given test point based on the labels of its nearest neighbours. The main difference between KNN regression and KNN classification is that KNN classifier returns the label that has the majority vote in the neighborhood, whilst KNN regressor returns the average of the neighborsâ€™ labels. \n",
    "\n",
    "#### Question 1 [KNN Regressor] \n",
    "Q1-1) Implement the KNN regressor function:\n",
    "                                     `knn(train.data, train.label, test.data, K=3)` \n",
    "which takes the training data and their labels (continuous values), the test set, and the size of the neighborhood (`K`). It should return the regressed values for the test data points. When choosing the neighbors, you can use the Euclidean distance function to measure the distance between a pair of data points. \n",
    "\n",
    "__Hint__: You are allowed to use KNN classifier code from Activity 1 of Module 1.\n",
    "\n",
    "Q1-2) Plot the training and the testing errors versus `1/K` for `K=1,..,20` in one plot, using the Task1A_train.csv and Task1A_test.csv datasets provided for this assignment. Discuss your findings.\n",
    "\n",
    "Q1-3) Report the best value for K in terms of the testing error. Discuss the values of K corresponding to underfitting and overfitting based on your plot in Q1-2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 [K-fold Cross Validation] \n",
    "Q2-1) Implement a K-fold Cross Validation (CV) function for your KNN regressor:  \n",
    "       `cv(train.data, train.label, numFold=10)` \n",
    "which takes the training data and their labels (continuous values), the number of folds, and returns RMSE for different folds of the training data. \n",
    "\n",
    "__Hint__: you are allowed to use bootstrap code from Activity 2 of Module 1.\n",
    "\n",
    "Q2-2) Using the training data, run your K-fold CV where the `numFold` is set to 10. Change the value of `K=1,..,20` and for each K compute the average `10` RMSE values you have got.  Plot the average error numbers versus `1/K` for `K=1,..,20`. Further, add two dashed lines around the average error indicating the average +/- standard deviation of errors. Include the plot in your report. \n",
    "\n",
    "Q2-3) Report the values of K that results the minimum average RMSE and minimum standard deviation of RMSE based on your cross validation plot in Q2-2.  Discuss your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 [KNN Regressor] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the R libraries required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2) # For plotting\n",
    "library(reshape2) # For ...\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-1 Implement the KNN regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to calculate the mean value of the K nearest neighbours based on a distance matrix. And a function to calculate the root mean square error (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn <- functionknn(train.data, train.label, test.data, K=3){\n",
    "    \n",
    "    # Pre-allocate vector and matrices if needed (optional)\n",
    "    # eg. for the best performance, find the distance between any pair of test and train set...\n",
    "    # and sort the index of train.data based on their distances to each test.data sample \n",
    "    \n",
    "    \n",
    "    # For each test sample do:\n",
    "    for (i in 1:nrow(nearest.indx.mtx)){\n",
    "        \n",
    "        # ...find its K nearest neighbours from training samples...\n",
    "        nn <- ...\n",
    "        #... and calculate the mean value of the K nearest neighbors\n",
    "        mean.value[i] <- ...\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Return the mean value as output\n",
    "    return (mean.value)\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "rmse <- function(real.value, estimated.value) {\n",
    "  return(...)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-2 Plot training and testing errors v.s. 1/K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data, then separate the predictors (train.data and test.data) from the target values (train.value and test.value) for input to the knn regressor function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train.data <- read.csv(\"../Task1A_train.csv\")\n",
    "test.data <- read.csv(\"../Task1A_test.csv\")\n",
    "\n",
    "# Split dependent and independent attributes\n",
    "train.label <- ...\n",
    "train.data <- ...\n",
    "\n",
    "test.label <- ...\n",
    "test.data <- ...\n",
    "\n",
    "train.len <- ...\n",
    "test.len <- ...\n",
    "\n",
    "# set random seed\n",
    "set.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the train and test RMSE's for K in 1:20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a dataframe to recors RMSE\n",
    "rmse.df <- data.frame('K'=1:20, 'TrainRMSE'=0, 'TestRMSE'=0)\n",
    "\n",
    "\n",
    "# calculating rmse... \n",
    "for (k in 1:20) {\n",
    "    \n",
    "    # training rmse\n",
    "    error[k, 'TrainRMSE'] = rmse(knn(...), ...)\n",
    "    \n",
    "    \n",
    "    # testing rmse\n",
    "    error[k, 'TestRMSE'] = rmse(knn(...), ...)\n",
    "        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the training and testing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape if needed\n",
    "...\n",
    "\n",
    "# Plot\n",
    "ggplot(data=..., aes(x=1/K, y=..., color=...)) + geom_line() +\n",
    "       scale_color_discrete(guide=guide_legend(title=NULL)) +\n",
    "       theme_minimal() +\n",
    "       scale_x_continuous(breaks=seq(0, 1, 0.1)) +\n",
    "       ggtitle(\"Train and Test RMSE's for Various 1/K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-3 Report the best K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We see ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 [K-fold Cross Validation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2-1 Implement a K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to segment a dataset into a given number of folds for K-fold cross validation to determine the most suitable value for the number of nearest neighbours K considering all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv <- function (train.data, train.label, num.fold=10, K=3){\n",
    "    # Initiate a dataframe to record RMSE\n",
    "    rmse.df <- data.frame('K'=1:K, 'L'=1:L, 'RMSE'=rep(0, L * K))\n",
    "    \n",
    "    dev.size = floor(... / L) # number of samples reserved for validation\n",
    "    # notice that since the sample size may not be a multiple of 10!\n",
    "    \n",
    "    for (l in 1:num.fold) {\n",
    "        dev.indices = ...\n",
    "        train.indices = ...\n",
    "        ...\n",
    "        \n",
    "        # for each value of k...\n",
    "        for (k in 1:K) {\n",
    "            ...\n",
    "            rmse.df[...,... ] = rmse(knn(...), ...)\n",
    "        }\n",
    "    ...\n",
    "    }\n",
    "    ...\n",
    "    return(rmse.df)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K <- 20 # maximum number of nearest neighbours\n",
    "L <- 10 # number of folds in cross validation\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2-2 Plot RMSE v.s. 1/K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the RMSE vs 1/K\n",
    "...\n",
    "\n",
    "ggplot(data=..., aes(x=1/K, y=...)) + geom_line() +\n",
    "       geom_line(data = ..., aes(x=1/K, y=...), linetype=\"dashed\") +\n",
    "       geom_line(data = ..., aes(x=1/K, y=...), linetype=\"dashed\") +\n",
    "       ggtitle(\"Mean RMSE +- sd v.s. 1/K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2-3 Report the best K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trend of the above curve shows ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B. Prediction Uncertainty with Bootstrapping\n",
    "This part is the adaptation of Activity 2 from KNN classification to KNN regression. You use the bootstrapping technique to quantify the uncertainty of predictions for the KNN regressor that you implemented in Part A. \n",
    "\n",
    "#### Question 3 [Bootstrapping]\n",
    "Q3-1) Modify the code in Activity 2 to handle bootstrapping for KNN regression. \n",
    "\n",
    "Q3-2) Load `Task1B_train.csv` and `Task1B_test.csv` sets. Apply your bootstrapping for KNN regression with `times = 100` (the number of subsets), `size = 25` (the size of each subset), and change `K=1,..,20` (the neighbourhood size). Now create a boxplot where the x-axis is `K`, and the y-axis is the average error (and the uncertainty around it) corresponding to each K.  \n",
    "\n",
    "Q3-3) Based on your plot in Q3-2, how does the test error and its uncertainty behave as `K` increases? \n",
    "\n",
    "Q3-4) Load `Task1B_train.csv` and `Task1B_test.csv` sets. Apply your bootstrapping for KNN regression with `K=5` (the neighbourhood size), `size = 25` (the size of each subset), and change `times = 10, 20, 30,.., 200` (the number of subsets). Now create a boxplot where the x-axis is `times`, and the y-axis is the average error (and the uncertainty around it) corresponding to each value of `times`.  \n",
    "\n",
    "Q3-5) Based on your plot in Q3-4, how does the test error and its uncertainty behave as the number of subsets in bootstrapping increases? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 [Bootstrapping] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3-1 Implement KNN regression with bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that randomly sample row indices with replacement from a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "boot <- function (original.size=100, sample.size=10, times=100){\n",
    "    \n",
    "    indx <- ...\n",
    "    for (t in 1:times){\n",
    "        \n",
    "        indx[t, ] <- sample(...)\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return(indx)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data, then separate the predictors from the target values for input to the knn regressor function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train.data <- read.csv(\"../Task1B_train.csv\")\n",
    "test.data <- read.csv(\"../Task1B_test.csv\")\n",
    "\n",
    "# Split dependent and independent attributes\n",
    "train.label <- ...\n",
    "train.data <- ...\n",
    "\n",
    "test.label <- ...\n",
    "test.data <- ...\n",
    "\n",
    "train.len <- ...\n",
    "test.len <- ...\n",
    "\n",
    "# set random seed\n",
    "set.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform the bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K <- 20             # Maximum K for KNN\n",
    "L <- 100            # Number of bootstrapped samples\n",
    "N <- 25             # Size of bootstrapped samples\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3-2 Plot bootstrapping KNN regression for different number of nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply bootstrapping for KNN regression with 100 bootstrapped datasets, \n",
    "# each having 25 samples, and maximum number of neighbours 20\n",
    "...\n",
    "\n",
    "ggplot(data=..., aes(..., ...,fill=Type)) + \n",
    "    geom_boxplot(outlier.shape = NA) + \n",
    "    scale_color_discrete(guide = guide_legend(title = NULL)) +\n",
    "    theme_minimal() +\n",
    "    ggtitle('RMSE by K (100 Bootstrap Samples for each K=1 to 20 with Size=25)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3-3 Results interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the boxplot..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3-4 Plot bootstrapping KNN regression for different number of bootstrapped datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): '...' used in an incorrect context\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): '...' used in an incorrect context\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "K <- 5                # k nearest neighbours\n",
    "N <- 25                # Size of bootstrapped samples\n",
    "max.sample.size = 200  # Maximum size of sampling\n",
    "\n",
    "# A dataframe to track the RMSE of each case\n",
    "rmse.df <- data.frame('Times'=..., 'RMSE'=0)\n",
    "# For a better permance do not repeat unnecessary experiments. \n",
    "# For example, if times=10 is already done, use its results and only do another \n",
    "# times=10 to achive times=20 and so on. \n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(data=..., aes(factor(Times), ...,fill=Type)) + \n",
    "    geom_boxplot(outlier.shape = NA) + \n",
    "    scale_color_discrete(guide = guide_legend(title = NULL)) +\n",
    "    theme_minimal() +\n",
    "    ggtitle('RMSE by Times (Times is the Number of Bootstrap Samples\\nof Size=25 with K = 10)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3-3 Results interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C. Probabilistic Machine Learning\n",
    "In this part, you show your knowledge about the foundation of the probabilistic machine learning (i.e. probabilistic inference and modeling) by solving one simple but basic statistical inference problems. Solve the following problem based on the probability concepts you have learned in Module 1 with the same math conventions. Please show your work in your report. Also, there are two conceptual questions.\n",
    "\n",
    "#### Question 4 [Bayes Rule] \n",
    "Recall the simple example from Appendix A of Module 1. Suppose we have one red and one blue box. In the red box we have 2 apples and 6 oranges, whilst in the blue box we have 3 apples and 1 orange. Now suppose we randomly selected one of the boxes and picked a fruit. If the picked fruit is an apple, what is the probability that it was picked from the blue box?\n",
    "\n",
    "Note that the chance of picking the red box is 40% and the selection chance for any of the pieces from a box is equal for all the pieces in that box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 4 [Bayes Rule] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_red = 0.40\n",
    "p_blue = ...\n",
    "\n",
    "...\n",
    "\n",
    "(p_blue_given_apple = ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above solution..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
